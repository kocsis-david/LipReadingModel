{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-20T10:16:48.818041Z",
     "start_time": "2024-05-20T10:16:48.804220Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D, MaxPooling3D,MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input, \\\n",
    "    ReLU, GlobalAveragePooling3D, add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "output_shape = 6\n",
    "input = Input(shape=(11, 60, 100, 1))\n",
    "\n",
    "'''block_0'''\n",
    "b0_conv3d_1 = Conv3D(64, kernel_size=(2, 3, 3),strides=(1,2,3), padding='same', use_bias=False,\n",
    "                     name='b0_conv3d_1', kernel_initializer='he_normal')(input)\n",
    "b0_relu_1 = ReLU(name='b0_relu_1')(b0_conv3d_1)\n",
    "b0_bn_1 = BatchNormalization(name='b0_bn_1')(b0_relu_1)\n",
    "\n",
    "\n",
    "'''block_1'''\n",
    "b1_cnv3d_1 = Conv3D(filters=16, kernel_size=(3, 3, 3) ,padding='same',\n",
    "                    use_bias=False, name='b1_cnv3d_1', kernel_initializer='he_normal')(b0_bn_1)\n",
    "b1_relu_1 = ReLU(name='b1_relu_1')(b1_cnv3d_1)\n",
    "b1_bn_1 = BatchNormalization(name='b1_bn_1')(b1_relu_1)  # size: 14*14\n",
    "\n",
    "b1_cnv3d_2 = Conv3D(filters=32, kernel_size=(1, 1, 1), padding='same',\n",
    "                    use_bias=False, name='b1_cnv3d_2', kernel_initializer='he_normal')(b1_bn_1)\n",
    "b1_relu_2 = ReLU(name='b1_relu_2')(b1_cnv3d_2)\n",
    "b1_out = BatchNormalization(name='b1_out')(b1_relu_2)  # size: 14*14\n",
    "\n",
    "'''block 2'''\n",
    "b2_cnv3d_1 = Conv3D(filters=32, kernel_size=(1, 1, 1), strides=(1, 1, 1), padding='same',\n",
    "                    use_bias=False, name='b2_cnv3d_1', kernel_initializer='he_normal')(b1_out)\n",
    "b2_relu_1 = ReLU(name='b2_relu_1')(b2_cnv3d_1)\n",
    "b2_bn_1 = BatchNormalization(name='b2_bn_1')(b2_relu_1)  # size: 14*14\n",
    "\n",
    "b2_add = add([b1_out, b2_bn_1])  #\n",
    "\n",
    "b2_cnv3d_2 = Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 3, 4),padding='same',\n",
    "                    use_bias=False, name='b2_cnv3d_2', kernel_initializer='he_normal')(b2_add)\n",
    "b2_relu_2 = ReLU(name='b2_relu_2')(b2_cnv3d_2)\n",
    "b2_out = BatchNormalization(name='b2_out')(b2_relu_2)  # size: 7*7\n",
    "\n",
    "'''block 3'''\n",
    "b3_cnv3d_1 = Conv3D(filters=64, kernel_size=(1, 1, 1), strides=(1, 1, 1), padding='same',\n",
    "                    use_bias=False, name='b3_cnv3d_1', kernel_initializer='he_normal')(b2_out)\n",
    "b3_relu_1 = ReLU(name='b3_relu_1')(b3_cnv3d_1)\n",
    "b3_bn_1 = BatchNormalization(name='b3_bn_1')(b3_relu_1)  # size: 7*7\n",
    "\n",
    "b3_add = add([b2_out, b3_bn_1])  #\n",
    "\n",
    "b3_cnv3d_2 = Conv3D(filters=64, kernel_size=(3, 3, 3), padding='same',\n",
    "                    use_bias=False, name='b3_cnv3d_2', kernel_initializer='he_normal')(b3_add)\n",
    "b3_relu_2 = ReLU(name='b3_relu_2')(b3_cnv3d_2)\n",
    "b3_out = BatchNormalization(name='b3_out')(b3_relu_2)  # size: 3*3\n",
    "\n",
    "'''block 4'''\n",
    "\n",
    "b4_cnv3d_1 = Conv3D(filters=64, kernel_size=(1, 1, 1), strides=(1, 1, 1), padding='same',\n",
    "                    use_bias=False, name='b4_cnv3d_1', kernel_initializer='he_normal')(b3_out)\n",
    "b4_relu_1 = ReLU(name='b4_relu_1')(b4_cnv3d_1)\n",
    "b4_bn_1 = BatchNormalization(name='b4_bn_1')(b4_relu_1)  # size: 7*7\n",
    "\n",
    "b4_add = add([b3_out, b4_bn_1])  #\n",
    "\n",
    "b4_cnv3d_2 = Conv3D(filters=128, kernel_size=(3, 3, 3), strides=(1, 2, 2), padding='same',\n",
    "                    use_bias=False, name='b4_cnv3d_2', kernel_initializer='he_normal')(b4_add)\n",
    "b4_relu_2 = ReLU(name='b4_relu_2')(b4_cnv3d_2)\n",
    "b4_out = BatchNormalization(name='b4_out')(b4_relu_2)\n",
    "\n",
    "\"\"\"BiLSTM\"\"\"\n",
    "\n",
    "reshaped2 = tf.keras.layers.Reshape((11,25* 128))(b4_out)\n",
    "bi_lstm =tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(reshaped2)\n",
    "dropout = tf.keras.layers.Dropout(0.5)(bi_lstm)\n",
    "bi_lstm2 =tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(dropout)\n",
    "dropout2 = tf.keras.layers.Dropout(0.5)(bi_lstm2)\n",
    "last_layer = Dense(128, activation='relu')(dropout2)\n",
    "output = Dense(output_shape, name='model_output', activation='softmax',\n",
    "               kernel_initializer='he_uniform')(last_layer)\n",
    "model = Model(input, output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.load_weights('/Users/koksziszdave/Egyetem/AIT/LipReadingModel/model.bilstm.weights.h5')",
   "id": "74990bbd2d81567d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "def getFrames(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def to_gray(frames):\n",
    "    gray_frames = []\n",
    "    for j in range(len(frames)):\n",
    "        gray_img = cv2.cvtColor(frames[j], cv2.COLOR_BGR2GRAY)\n",
    "        gray_img = cv2.resize(gray_img, (gray_img.shape[1] ,  gray_img.shape[0]))\n",
    "        gray_frames.append(gray_img)\n",
    "        \n",
    "    return gray_frames\n",
    "\n",
    "def face_detect(face_classifier, images):\n",
    "   \n",
    "    detected_faces = []\n",
    "    for j in range(len(images)):\n",
    "        face = face_classifier.detectMultiScale(\n",
    "                images[j], scaleFactor=1.1, minNeighbors=5, minSize=(40, 40)\n",
    "        )\n",
    "        if len(face) == 1:\n",
    "            for (x, y, w, h) in face:\n",
    "                cv2.rectangle(images[j], (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "                detected_faces.append((x, y, w, h))\n",
    "        \n",
    "    \n",
    "    \n",
    "    return detected_faces\n",
    "\n",
    "from imutils import face_utils\n",
    "import dlib\n",
    "import numpy as np\n",
    "def lip_detect(images, og_images, predictor):\n",
    "    \n",
    "    lips_list = []\n",
    "    for j in range(len(images)):\n",
    "        frame = og_images[j]\n",
    "        x, y, w, h = images[j] \n",
    "        face_box = dlib.rectangle(left=x, top=y, right=x + w, bottom=y + h)\n",
    "        shape = predictor(frame, face_box) \n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        \n",
    "        (x, y, w, h) = cv2.boundingRect(np.array([shape[48:68]]))\n",
    "        margin = 10\n",
    "        lips = frame[y-margin:y+h+margin, x-margin:x+w+margin]\n",
    "        lips = cv2.resize(lips,(100,60))\n",
    "        lips_list.append(lips)\n",
    "        \n",
    "    return lips_list\n",
    "\n",
    "def get_middle_frames(lips, frame_num):\n",
    "    separator = (29 - frame_num) // 2\n",
    "    middle_frames = lips[separator:separator + frame_num]\n",
    "    return middle_frames\n",
    "\n",
    "import csv\n",
    "def load_labels():\n",
    "    with open('labels.csv', mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        row = next(reader)\n",
    "        \n",
    "    return row\n",
    "\n",
    "def predict(frames, model):\n",
    "    frames = np.array(frames)\n",
    "    print(frames.shape)\n",
    "    frames = frames / 255.0\n",
    "    frames = frames.reshape(1,11, 60, 100, 1)\n",
    "    prediction = model.predict(frames)\n",
    "    print(prediction)\n",
    "    idx = np.argmax(prediction[0])\n",
    "    print(idx)\n",
    "    labels=load_labels()\n",
    "    \n",
    "    \n",
    "    return labels[idx]\n",
    "\n",
    "def getWord(video):\n",
    "    frames=getFrames(video)\n",
    "    \n",
    "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    detectedfacebb=face_detect(face_classifier,frames)\n",
    "    \n",
    "    \n",
    "    predictor_path = r'/Users/koksziszdave/Egyetem/AIT/Models/shape_predictor_68_face_landmarks.dat'\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    lips=lip_detect(detectedfacebb,frames,predictor)\n",
    "    gray_lips=to_gray(lips)\n",
    "    \n",
    "    final_frames=get_middle_frames(gray_lips,11)\n",
    "    word=predict(final_frames,model)\n",
    "    \n",
    "    return  word\n",
    "    \n"
   ],
   "id": "4659501b228f2cad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T10:17:03.114975Z",
     "start_time": "2024-05-20T10:16:59.715573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "demo = gr.Interface(fn=getWord, inputs=\"video\", outputs=\"text\", title=\"Lip Reading Model\", description=\"This model can predict the word from a video of a person speaking.\")\n",
    "\n",
    "demo.launch(share=True)  # Share your demo with just 1 extra parameter ðŸš€"
   ],
   "id": "1872300011d43a05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://f5a11ce8ab91c7991c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://f5a11ce8ab91c7991c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f07b25060a3daed2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
